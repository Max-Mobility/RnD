<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Product Overview | rnd</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Product Overview" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Idea" />
<meta property="og:description" content="Idea" />
<meta property="og:site_name" content="rnd" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-02T14:00:54-05:00" />
<script type="application/ld+json">
{"description":"Idea","@type":"BlogPosting","url":"/RnD/mobility-unlimited/product-overview.html","headline":"Product Overview","dateModified":"2018-08-02T14:00:54-05:00","datePublished":"2018-08-02T14:00:54-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/RnD/mobility-unlimited/product-overview.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/RnD/assets/main.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/><link type="application/atom+xml" rel="alternate" href="/RnD/feed.xml" title="rnd" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/RnD/">rnd</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/RnD/documentation.html">Documentation Guide</a><a class="page-link" href="/RnD/gazebo-simulation.html">Gazebo Simulation</a><a class="page-link" href="/RnD/gaze-tracking.html">Gaze Tracking</a><a class="page-link" href="/RnD/SIM.html">Spherical ACIM</a><a class="page-link" href="/RnD/mobility-unlimited.html">Mobility Unlimited</a><a class="page-link" href="/RnD/roomba.html">Roomba</a><a class="page-link" href="/RnD/autochair.html">Autochair</a><a class="page-link" href="/RnD/openpose.html">OpenPose</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Product Overview</h1>
  </header>

  <div class="post-content">
    <h2 id="idea">Idea</h2>

<p>A <em>smart joystick</em> that aids the user in navigating their world</p>

<p>Specifically a <em>plug-and-play (PnP)</em> device targeting people with ALS to
provide a sliding scale of support throughout their lives.</p>

<p>This device will:</p>

<ol>
  <li>Implement alternative controls with scaling complexity</li>
  <li>Identify and avoid obstacles</li>
  <li>Infer the intent of the user</li>
  <li>Identify and react to people</li>
  <li>Share control of the chair between the user and onboard
systems</li>
</ol>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">What does it mean to react to a person?</p><p>In its simplest form, people can be treated as just another obstacle
– something to be navigated around in the most efficient or
quickest way possible.</p>

<p>But in reality, this is not feasible. An individual has their own
<code class="highlighter-rouge">social space</code> to be considered. In most cases it is better to
travel behind an individual than in front when moving around
them. If two or more individuals are having a conversation, it would
be very rude to force our way between them.</p>

<p>Additionally, sometimes a person might not be just another obstacle,
but instead the user’s goal destination. We might want to approach
them or follow at a set distance.</p>

<p>All of this implies an additional,capability of our device:
<code class="highlighter-rouge">contextual awareness</code>. This essentially is the fusion of all of our
stated capabilities: user input via various controls, estimation of
user intent, and identification of individuals in the environment,
to allow the system to make informed decisions.</p>


</div></div>
<p>The sensor stack on the device wil be capable of meeting all of the
individual’s control input needs, from initially being capable of
operating a standard inductive joystick, to control through head
movement, all the way to gaze detection.</p>

<h2 id="novelty">Novelty</h2>

<p>This product wil stand out in several ways:</p>

<ol>
  <li>Novel, low-cost implementation of advanced procedures such as
gaze-tracking and obstacle detection</li>
  <li>Novel techniques for <code class="highlighter-rouge">collaboration</code> between the user and device.</li>
</ol>

<h2 id="technology">Technology</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Type</th>
      <th>Description</th>
      <th>Quantity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Jetson TX2</td>
      <td>Embedded System</td>
      <td>The brains of the unit. Conducts all onboard processing, from eye-gaze tracking to people dectection to obstacle avoidance</td>
      <td>1</td>
    </tr>
    <tr>
      <td>User-Facing Camera</td>
      <td>Sensor</td>
      <td>Provides a video stream of the user for use in affect and intent determination. Starting with eye-gaze for now.</td>
      <td>At least 1</td>
    </tr>
    <tr>
      <td>Forward-Facing Camera</td>
      <td>Sensor</td>
      <td>Provides a video stream for obstacle and people detection.</td>
      <td>At least 1</td>
    </tr>
    <tr>
      <td>IMU</td>
      <td>Sensor</td>
      <td>6-9 axis imu for use in sensor fusion processes. Possibly useful for path planning and intent detection.</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Inductive Joystick</td>
      <td>Joystick</td>
      <td>Typical JSM joystick for controlling the chair</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">RGB and FLIR Cameras</p><p>The cheapest and easiest solution for our image processing needs is
the standard RGB camera. Unfortunately, the performance and
reliability of an RGB camera can degrade when in extremely bright or
dark conditions
A solution to this is the use of a FLIR thermal camera. This type of
camera is resistant to glare and lighting changes, and can provide
another stream of data for our image processing. The caveat is the
addition of another, more expensive camera.</p>


</div></div>
<h2 id="alternative-controls">Alternative Controls</h2>
<ul>
  <li>Joystick Semi-Autonomy</li>
  <li>Head-Tilt Control</li>
  <li>Eye-Gaze Control</li>
</ul>

<p>These control aspects will function from the same input device, which
will adapt alongside the user so that they can learn and use a single
input device as their ability changes. Such adaptability will allow
decrease the burden on the healthcare system, the technicians, and the
clinicians who would normally have to fund, provide, configure, and
manage the transition between different input devices. Additionally,
by allowing the user access to more capable control inputs which can
keep them in control of their chair for longer, they will be able to
maintain their independence and their mobility for longer, thus
increasing their quality of life.</p>

<h3 id="joystick-semi-autonomy">Joystick Semi-Autonomy</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>This is the most basic mode of collaborative control. The user
steers with the joystick as usual, but the obstacle detection and
environmental awareness of the device aid in obstacle avoidance.
Use techniques such as a vector field histogram and shared control</p>


</div></div>
<p>The semi-autonomous joystick input mode provides a superior level of
control and independence over a standard joystick and is designed to
help the user better navigate the world around them. Associated with
navigation is the cognitive load of keeping track of the changing
environment around you, which can be especially difficult and
near-impossible because the chair itself obstructs your view and
hearing of the world. The smart joystick takes care of these issues by
allowing the user to give the general intention - that is the
direction they want to go or the person they want to follow - and the
chair can (optionally) manage the speed and steering adjustments to
keep them on that course. Such a feature is useful for easily
navigating up/down a ramp, through a doorway or hallway, or along a
sidewal. In such scenarios, the smart joystick can automatically
adjust for environmental conditions such as cross-slope or obstacles.</p>

<h3 id="head-tilt-control">Head-Tilt Control</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Everything the joystick does, but with an additiona layer of Open
Pose that translates the user’s head-neck orientation into a
joystick command. The larger uncertainty in this control input
should mean that the autonomous system’s control inputs should have
a larger weight in the shared control scheme</p>


</div></div>
<h3 id="eye-gaze-control">Eye-Gaze Control</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>The most advanced user interface. A first incarnation would be
controlling buttons on a screen, but more advanced work and
interfaces can be made to allow more intuitive eye control. All
autonomy/safety systems active the same as other input schemes.</p>


</div></div>
<h2 id="environment-detection">Environment Detection</h2>

<p>We will use a monocular vision approach to generate both visual
odometry and a depth map of the environment. Long term, this data can
be used in a SLAM algorithm to map and localize the chair in the
environment. Short term, the depth map and odometry can be used to
locate obstacles and people in the immediate vicinity and allow the
system to react accordingly.</p>

<h3 id="depth-map-generation">Depth Map Generation</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Monocular Depth Estimation via a deep model
https://arxiv.org/abs/1806.01260</p>


</div></div>
<h3 id="visual-odometry">Visual Odometry</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>UnDeepVO: Monocular Visual Odometry through Unsupervised Deep
Learning
http://senwang.gitlab.io/UnDeepVO/</p>


</div></div>
<h3 id="obstacle-detection">Obstacle Detection</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Using image segmentation on the depth map to identify nearby
obstacles and drivable surfaces</p>


</div></div>
<h3 id="people-detection">People Detection</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Use of OpenPose for detecting people within the video’s frame.</p>


</div></div>
<h2 id="contextual-awareness">Contextual Awareness</h2>

<p><code class="highlighter-rouge">Contextual Awareness</code> is the process of informed decision making in
regards to intent determination. This involves first identifying the
user’s actions and then trying to determine a best course of action
based on the environment and these actions. This contextual awareness
is what will help determine the amount of autonomy vs manual control
of the system at any point.</p>

<p>For example, say that a user indicates that they want to move forward,
if the environment is free of obstacles, the system should allow
mainly manual control of the system, and let the user move forward of
their own accord, even if the actual path traveled ends up being
slightly meandering. However, if there is a large curb or people
around the chair, the chair should provide a larger level of
autonomous control (while still considering the user’s input!) to
ensure no collections occur.</p>

<h3 id="intent-determination">Intent Determination</h3>

<p>Based on the sensor inputs available (joystick/head-tilt/eye-gaze and
camera feeds) general guesses can be made to the user’s intent. This
can be something as low level as <code class="highlighter-rouge">move around this obstacle</code>, <code class="highlighter-rouge">go
through this doorway</code>, or as high level as <code class="highlighter-rouge">follow alongside this
person</code>.</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Talk about determination of the user’s goal or intent based on the
data available. Example being using the joystick’s input to determine
the goal location to travel towards, and incorporating the
gaze-direction to identify targets such as “go through a doorway” or
“approach this person”.</p>


</div></div>
<h3 id="user-device-collaboration">User-Device Collaboration</h3>
<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Essentially an explanation of Human-Robot interaction and how this
is a critical element to allow the user to feel as independent as
possible</p>


</div></div>
<h2 id="target-demonstrables-for-finalist-application">Target Demonstrables for Finalist Application</h2>

<p>The goal for the finalist application should be to demonstrate working
prototypes of key system components, so that it can be emphasized that
the potential for our product is high and definitely realizable. This
will allow us to emphasize that the majority of the work is
integration of these technologies into a joystick’s form factor and
then extensive work on intent determination, shared control and user interface.</p>

<ol>
  <li>Eye Gaze Determination</li>
  <li>People Detection</li>
  <li>Depth Map Generation</li>
  <li>Plug-N-Play Capability</li>
</ol>

<h2 id="notes-for-things-to-talk-about-in-preceding-sections">Notes for things to talk about in preceding sections</h2>

<ul>
  <li>Affect Determination</li>
  <li>Replacement of lidar/rgb-d cameras with monocular camera and ML</li>
  <li>ML for depth map generation</li>
  <li>Better distinguish what are the novel elements of the product. The
novel elements will be the main emphasis when presenting for the
application, all well as demonstrating the overlal potential for the
device to aid users.</li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/RnD/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">rnd</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">rnd</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Centralized repo for R&amp;D documentation</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
