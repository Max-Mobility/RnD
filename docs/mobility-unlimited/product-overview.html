<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Product Overview | rnd</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Product Overview" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Idea" />
<meta property="og:description" content="Idea" />
<meta property="og:site_name" content="rnd" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-01T22:08:20-05:00" />
<script type="application/ld+json">
{"description":"Idea","@type":"BlogPosting","url":"/RnD/mobility-unlimited/product-overview.html","headline":"Product Overview","dateModified":"2018-08-01T22:08:20-05:00","datePublished":"2018-08-01T22:08:20-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/RnD/mobility-unlimited/product-overview.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/RnD/assets/main.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/><link type="application/atom+xml" rel="alternate" href="/RnD/feed.xml" title="rnd" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/RnD/">rnd</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/RnD/documentation.html">Documentation Guide</a><a class="page-link" href="/RnD/gazebo-simulation.html">Gazebo Simulation</a><a class="page-link" href="/RnD/gaze-tracking.html">Gaze Tracking</a><a class="page-link" href="/RnD/SIM.html">Spherical ACIM</a><a class="page-link" href="/RnD/mobility-unlimited.html">Mobility Unlimited</a><a class="page-link" href="/RnD/roomba.html">Roomba</a><a class="page-link" href="/RnD/autochair.html">Autochair</a><a class="page-link" href="/RnD/openpose.html">OpenPose</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Product Overview</h1>
  </header>

  <div class="post-content">
    <h2 id="idea">Idea</h2>

<p>A <em>smart joystick</em> that aids the user in navigating their world.</p>

<p>Specifically a <em>plug-n-play</em> device targeting people with ALS to
provide a sliding scale of support throughout their lives.</p>

<p>This device will:</p>

<ol>
  <li>Implement alternative controls with scaling complexity</li>
  <li>Identify and avoid obstacles</li>
  <li>Infer the intent of the user</li>
  <li>Identify and react to people</li>
  <li>Share control of the chair between the user and onboard
systems</li>
</ol>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">What does it mean to react to a person?</p><p>In its simplest form, people can be treated as just another obstacle
– something to be navigated around in the most efficient or
quickest way possible.</p>

<p>But in reality, this is not feasible. An individual has their own
<code class="highlighter-rouge">social space</code> to be considered. In most cases it is better to
travel behind an individual than in front when moving around
them. If two or more individuals are having a conversation, it would
be very rude to force our way between them.</p>

<p>Additionally, sometimes a person might not be just another obstacle,
but instead the user’s goal destination. We might want to approach
them or follow at a set distance.</p>

<p>All of this implies an additional,capability of our device:
<code class="highlighter-rouge">contextual awareness</code>. This essentially is the fusion of all of our
stated capabilities: user input via various controls, estimation of
user intent, and identification of individuals in the environment,
to allow the system to make informed decisions.</p>


</div></div>
<p>The sensor stack on the device wil be capable of meeting all of the
individual’s control input needs, from initially being capable of
operating a standard inductive joystick, to control through head
movement, all the way to gaze detection.</p>

<h2 id="novelty">Novelty</h2>

<p>This product wil stand out in several ways:</p>

<ol>
  <li>Novel, low-cost implementation of advanced procedures such as
gaze-tracking and obstacle detection.</li>
  <li>Novel techniques for <code class="highlighter-rouge">collaboration</code> between the user and device.</li>
</ol>

<h2 id="technology">Technology</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Type</th>
      <th>Description</th>
      <th>Quantity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>*Jetson TX2</td>
      <td>Embedded System</td>
      <td>The brains of the unit. Conducts all onboard processing, from eye-gaze tracking to people dectection to obstacle avoidance</td>
      <td>1</td>
    </tr>
    <tr>
      <td>*User-Facing Camera</td>
      <td>Sensor</td>
      <td>Provides a video stream of the user for use in affect and intent determination. Starting with eye-gaze for now.</td>
      <td>At least 1</td>
    </tr>
    <tr>
      <td>*Forward-Facing Camera</td>
      <td>Sensor</td>
      <td>Provides a video stream for obstacle and people detection.</td>
      <td>At least 1</td>
    </tr>
    <tr>
      <td>*IMU</td>
      <td>Sensor</td>
      <td>6-9 axis imu for use in sensor fusion processes. Possibly useful for path planning and intent detection.</td>
      <td>1</td>
    </tr>
    <tr>
      <td>*Inductive Joystick</td>
      <td>Joystick</td>
      <td>Typical JSM joystick for controlling the chair</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">RGB and FLIR Cameras</p><p>The cheapest and easiest solution for our image processing needs is
the standard RGB camera. Unfortunately, the performance and
reliability of an RGB camera can degrade when in extremely bright or
dark conditions.<br />
A solution to this is the use of a FLIR thermal camera. This type of
camera is resistant to glare and lighting changes, and can provide
another stream of data for our image processing. The caveat is the
addition of another, more expensive camera.</p>


</div></div>
<h2 id="alternative-controls">Alternative Controls</h2>
<ul>
  <li>Joystick Semi-Autonomy</li>
  <li>Head-Tilt Control</li>
  <li>Eye-Gaze Control</li>
</ul>

<h3 id="joystick-semi-autonomy">Joystick Semi-Autonomy</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>This is the most basic mode of collaborative control. The user
steers with the joystick as usual, but the obstacle detection and
environmental awareness of the device aid in obstacle avoidance.
Use techniques such as a vector field histogram and shared control</p>


</div></div>
<h3 id="head-tilt-control">Head-Tilt Control</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Everything the joystick does, but with an additiona layer of Open
Pose that translates the user’s head-neck orientation into a
joystick command. The larger uncertainty in this control input
should mean that the autonomous system’s control inputs should have
a larger weight in the shared control scheme</p>


</div></div>
<h3 id="eye-gaze-control">Eye-Gaze Control</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>The most advanced user interface. A first incarnation would be
controlling buttons on a screen, but more advanced work and
interfaces can be made to allow more intuitive eye control. All
autonomy/safety systems active the same as other input schemes.</p>


</div></div>
<h2 id="environment-detection">Environment Detection</h2>

<p>We will use a monocular vision approach to generate both visual
odometry and a depth map of the environment. Long term, this data can
be used in a SLAM algorithm to map and localize the chair in the
environment. Short term, the depth map and odometry can be used to
locate obstacles and people in the immediate vicinity and allow the
system to react accordingly.</p>

<h3 id="depth-map-generation">Depth Map Generation</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Monocular Depth Estimation via a deep model 
https://arxiv.org/abs/1806.01260</p>


</div></div>
<h3 id="visual-odometry">Visual Odometry</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>UnDeepVO: Monocular Visual Odometry through Unsupervised Deep
Learning
http://senwang.gitlab.io/UnDeepVO/</p>


</div></div>
<h3 id="obstacle-detection">Obstacle Detection</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Using image segmentation on the depth map to identify nearby
obstacles and drivable surfaces</p>


</div></div>
<h3 id="people-detection">People Detection</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Use of OpenPose for detecting people within the video’s frame.</p>


</div></div>
<h2 id="contextual-awareness">Contextual Awareness</h2>

<p><code class="highlighter-rouge">Contextual Awareness</code> is the process of informed decision making in
regards to intent determination</p>

<h3 id="intent-determination">Intent Determination</h3>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Talk about determination of the user’s goal or intent based on the
data available. Example being using the joystick’s input to determine
the goal location to travel towards, and incorporating the
gaze-direction to identify targets such as “go through a doorway” or
“approach this person”.</p>


</div></div>
<h3 id="affect-recognition">Affect Recognition</h3>
<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>I think this aspect of the device should be nixxed for now. Not
enough examples of usage and how it improves control.</p>

<p>Talk about higher-level techniques that can be incorporated since we
have a camera facing the user and are already actively analyzing
their face. Determination of aspects such as focus, mood, stress,
etc. Provides more context for decision making.<br />
Example: Smiling while eye-gaze is targetting a person and the
joystick is pointed towards them indicates that the user’s intent is
to approach the individual.</p>


</div></div>
<h3 id="user-device-collaboration">User-Device Collaboration</h3>
<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">TODO</p><p>Essentially an explanation of Human-Robot interaction and how this
is a critical element to allow the user to feel as independent as
possible</p>


</div></div>
<h2 id="target-demonstrables-for-finalist-application">Target Demonstrables for Finalist Application</h2>

<p>The goal for the finalist application should be to demonstrate working
prototypes of key system components, so that it can be emphasized that
the potential for our product is high and definitely realizable. This
will allow us to emphasize that the majority of the work is
integration of these technologies into a joystick’s form factor and
then extensive work on intent determination, shared control and user interface.</p>

<ol>
  <li>Eye Gaze Determination</li>
  <li>People Detection</li>
  <li>Depth Map Generation</li>
  <li>Plug-N-Play Capability</li>
</ol>

<h2 id="notes-for-things-to-talk-about-in-preceding-sections">Notes for things to talk about in preceding sections</h2>

<ul>
  <li>Affect Determination</li>
  <li>Replacement of lidar/rgb-d cameras with monocular camera and ML</li>
  <li>ML for depth map generation</li>
  <li>Better distinguish what are the novel elements of the product. The
novel elements will be the main emphasis when presenting for the
application, all well as demonstrating the overlal potential for the
device to aid users.</li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/RnD/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">rnd</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">rnd</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Centralized repo for R&amp;D documentation</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
