<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Progress Update 7/3/18 | rnd</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Progress Update 7/3/18" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Progress Update 7/3/18 Built custom pre-processor to prepare data to feed into machine learning model Handles all steps from taking raw data to feeding to model Provides ability to the data being fed to model Made project portable to make it easy to run on various machines Tested execution on small dataset (5 subjects, ~100 pictures) Currently testing execution and training on full dataset (1474 subjects, ~2.5 million pictures) to replicate results found in Eye tracking for everyone (Krafka et al., 2016) Created documentation for setup and execution of the program located here" />
<meta property="og:description" content="Progress Update 7/3/18 Built custom pre-processor to prepare data to feed into machine learning model Handles all steps from taking raw data to feeding to model Provides ability to the data being fed to model Made project portable to make it easy to run on various machines Tested execution on small dataset (5 subjects, ~100 pictures) Currently testing execution and training on full dataset (1474 subjects, ~2.5 million pictures) to replicate results found in Eye tracking for everyone (Krafka et al., 2016) Created documentation for setup and execution of the program located here" />
<meta property="og:site_name" content="rnd" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-02T14:00:54-05:00" />
<script type="application/ld+json">
{"description":"Progress Update 7/3/18 Built custom pre-processor to prepare data to feed into machine learning model Handles all steps from taking raw data to feeding to model Provides ability to the data being fed to model Made project portable to make it easy to run on various machines Tested execution on small dataset (5 subjects, ~100 pictures) Currently testing execution and training on full dataset (1474 subjects, ~2.5 million pictures) to replicate results found in Eye tracking for everyone (Krafka et al., 2016) Created documentation for setup and execution of the program located here","@type":"BlogPosting","url":"/RnD/gaze-tracking/progress_update7-3-18.html","headline":"Progress Update 7/3/18","dateModified":"2018-08-02T14:00:54-05:00","datePublished":"2018-08-02T14:00:54-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/RnD/gaze-tracking/progress_update7-3-18.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/RnD/assets/main.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/><link type="application/atom+xml" rel="alternate" href="/RnD/feed.xml" title="rnd" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/RnD/">rnd</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/RnD/documentation.html">Documentation Guide</a><a class="page-link" href="/RnD/gazebo-simulation.html">Gazebo Simulation</a><a class="page-link" href="/RnD/gaze-tracking.html">Gaze Tracking</a><a class="page-link" href="/RnD/SIM.html">Spherical ACIM</a><a class="page-link" href="/RnD/mobility-unlimited.html">Mobility Unlimited</a><a class="page-link" href="/RnD/roomba.html">Roomba</a><a class="page-link" href="/RnD/autochair.html">Autochair</a><a class="page-link" href="/RnD/openpose.html">OpenPose</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Progress Update 7/3/18</h1>
  </header>

  <div class="post-content">
    <h1 id="progress-update-7318">Progress Update 7/3/18</h1>
<ul>
  <li>Built custom pre-processor to prepare data to feed into machine learning model
    <ul>
      <li>Handles all steps from taking raw data to feeding to model</li>
      <li>Provides ability to the data being fed to model</li>
    </ul>
  </li>
  <li>Made project portable to make it easy to run on various machines</li>
  <li>Tested execution on small dataset (5 subjects, ~100 pictures)</li>
  <li>Currently testing execution and training on full dataset (1474 subjects, ~2.5 million pictures) to replicate results found in <a href="http://gazecapture.csail.mit.edu/">Eye tracking for everyone</a> (Krafka et al., 2016)</li>
  <li>Created documentation for setup and execution of the program located <a href="https://github.com/SwapnilPande/GazeTracking/blob/master/iTracker/README.md">here</a></li>
</ul>

<h1 id="next-steps">Next Steps</h1>
<ul>
  <li>Finish training model on Williamâ€™s computer and measure model performance</li>
  <li>Train the model on the DGX-1 for a longer duration of time</li>
  <li>Add the ability to save current training progress so that execution can be stopped and resumed without having to restart training</li>
  <li>Remove the requirement to unzip data every execution</li>
  <li>Create visualizer to visualize the output of the model
    <ul>
      <li>Should be able to see actual gaze direction and predicted gaze direction for a given picture</li>
    </ul>
  </li>
  <li>Feed new gaze data (not included in dataset) to model and test performance</li>
  <li>Modify ML model to produce the gaze vector, rather than gaze coordinates</li>
  <li>Iterate and improve peformance</li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/RnD/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">rnd</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">rnd</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Centralized repo for R&amp;D documentation</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
